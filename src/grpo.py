import torch
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Optional
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model
import copy


class GRPOLoss:
    """
    GRPO (Generative Reward-Powered Optimization) loss implementation.
    
    This class implements the GRPO loss function which combines:
    1. Policy gradient loss with clipping
    2. KL divergence penalty to stay close to reference model
    """
    
    def __init__(self, epsilon: float = 0.2, beta: float = 0.1):
        """
        Initialize GRPO loss.
        
        Args:
            epsilon: Clipping parameter for policy ratio
            beta: KL divergence penalty weight
        """
        self.epsilon = epsilon
        self.beta = beta
    
    def prepare_inputs(self, tokenizer, prompt: str, completion: str) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Prepare inputs for loss computation.
        
        Args:
            tokenizer: Tokenizer for text processing
            prompt: Input prompt
            completion: Generated completion
            
        Returns:
            input_ids: Tokenized input sequence
            attention_mask: Attention mask
            completion_mask: Mask for completion tokens
        """
        # Tokenization
        prompt_tokens = tokenizer(prompt, return_tensors="pt")
        completion_tokens = tokenizer(completion, return_tensors="pt")

        # Combined input
        input_ids = torch.cat(
            [
                prompt_tokens["input_ids"],
                completion_tokens["input_ids"]
            ], 
            dim=1
        )
        attention_mask = torch.cat(
            [
                prompt_tokens["attention_mask"],
                completion_tokens["attention_mask"]
            ],
            dim=1
        )

        prompt_length = prompt_tokens["input_ids"].shape[1]
        completion_length = completion_tokens["input_ids"].shape[1]
        total_length = prompt_length + completion_length

        # Create a mask to identify the tokens that 
        # were generated by the model in the full sequence
        completion_mask = torch.zeros(total_length, dtype=torch.float32)
        completion_mask[prompt_length:] = 1.0

        return input_ids, attention_mask, completion_mask
    
    def compute_log_probs(self, model, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """
        Compute log probabilities for each token in the sequence.
        
        Args:
            model: Language model
            input_ids: Input token IDs
            attention_mask: Attention mask
            
        Returns:
            Log probabilities for each token
        """
        outputs = model(input_ids, attention_mask=attention_mask)
        
        # Computing the log-probability of each token in the sequence
        log_probs = F.log_softmax(outputs.logits, dim=-1)
        
        # Extract the log-probability for the actual token that 
        # was generated at each position in the sequence.
        return log_probs.gather(
            dim=-1, 
            index=input_ids.unsqueeze(-1)
        ).squeeze(-1)
    
    def __call__(self, model, ref_model, tokenizer, prompt: str, completion: str, advantage: float) -> torch.Tensor:
        """
        Compute GRPO loss.
        
        Args:
            model: Policy model (being trained)
            ref_model: Reference model (fixed)
            tokenizer: Tokenizer for text processing
            prompt: Input prompt
            completion: Generated completion
            advantage: Advantage/reward value
            
        Returns:
            Loss tensor
        """
        input_ids, attention_mask, completion_mask = self.prepare_inputs(
            tokenizer, prompt, completion
        )

        # Model forward
        token_log_probs = self.compute_log_probs(
            model, input_ids, attention_mask
        )
        with torch.no_grad():
            ref_token_log_probs = self.compute_log_probs(
                ref_model, input_ids, attention_mask
        )

        # ratio = p_model / p_ref = exp(log(p_model) - log(p_ref))
        ratio = torch.exp(token_log_probs - ref_token_log_probs)

        # Scale the ratio by the advantage function
        unclipped = ratio * advantage
        clipped = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantage

        policy_loss = torch.min(unclipped, clipped)

        # Compute the per-token KL divergence to encourage the model 
        # to stay close to the reference model
        delta = token_log_probs - ref_token_log_probs
        per_token_kl = torch.exp(-delta) + delta - 1

        # We want to maximize reward, so we make the loss negative 
        # because optimizers minimize loss.
        per_token_loss = -(policy_loss - self.beta * per_token_kl)

        # Only compute loss over the output tokens
        loss = (per_token_loss * completion_mask).sum() / completion_mask.sum()
        return loss


class GRPOTrainer:
    """
    Trainer for GRPO fine-tuning.
    """
    
    def __init__(self, model_str: str, lora_config: Optional[LoraConfig] = None):
        """
        Initialize GRPO trainer.
        
        Args:
            model_str: Model identifier
            lora_config: LoRA configuration (if None, uses default)
        """
        self.model_str = model_str
        self.base_model = AutoModelForCausalLM.from_pretrained(model_str)
        self.tokenizer = AutoTokenizer.from_pretrained(model_str)
        
        # Configure tokenizer
        self.tokenizer.padding_side = "left"
        self.tokenizer.truncation_side = "left"
        
        # Create reference model (frozen copy)
        self.ref_model = copy.deepcopy(self.base_model)
        self.ref_model.eval()
        
        # Setup LoRA if config provided
        if lora_config is None:
            lora_config = LoraConfig(
                r=8,
                lora_alpha=32,
                target_modules=["q_proj", "v_proj"],
                lora_dropout=0.1,
                init_lora_weights=False,
                bias="none",
                task_type="CAUSAL_LM"
            )
        
        # Apply LoRA to model
        self.model = get_peft_model(self.base_model, lora_config)
        
        # Initialize loss function
        self.loss_fn = GRPOLoss()
    
    def compute_loss(self, prompt: str, completion: str, advantage: float) -> torch.Tensor:
        """
        Compute GRPO loss for a single example.
        
        Args:
            prompt: Input prompt
            completion: Generated completion
            advantage: Advantage/reward value
            
        Returns:
            Loss tensor
        """
        return self.loss_fn(
            self.model, 
            self.ref_model, 
            self.tokenizer, 
            prompt, 
            completion, 
            advantage
        )
    
    def generate_completion(self, prompt: str, max_new_tokens: int = 10) -> str:
        """
        Generate completion for a prompt.
        
        Args:
            prompt: Input prompt
            max_new_tokens: Maximum number of new tokens to generate
            
        Returns:
            Generated completion
        """
        input_ids = self.tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **input_ids,
                max_new_tokens=max_new_tokens,
                pad_token_id=self.tokenizer.pad_token_id
            )
        
        generated_text = self.tokenizer.decode(
            outputs[0], skip_special_tokens=True
        )
        return generated_text[len(prompt):]
    
    def get_trainable_parameters(self):
        """Get trainable parameters (LoRA parameters only)."""
        return self.model.parameters() 